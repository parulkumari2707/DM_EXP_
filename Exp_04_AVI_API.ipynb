{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJoAo4NCu2BhG2IBRTRN8B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EeQbqQKgRZap","executionInfo":{"status":"ok","timestamp":1716750157141,"user_tz":-330,"elapsed":660,"user":{"displayName":"Mohammed Mandekar","userId":"14176810042991264297"}},"outputId":"0a9c39ce-45a8-4e8b-9bde-f3215f7ef936"},"outputs":[{"output_type":"stream","name":"stdout","text":["Approximate Value Iteration (AVI) Policy:\n","[0 0 0 0 0]\n","Approximate Policy Iteration (API) Policy:\n","[0 0 0 0 0]\n"]}],"source":["import numpy as np\n","\n","# AVI implementation\n","class ApproximateValueIteration:\n","    def __init__(self, state_dim, action_dim, feature_dim, gamma=0.9, epsilon=1e-6, max_iterations=1000):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.feature_dim = feature_dim\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.max_iterations = max_iterations\n","        self.weights = np.zeros((action_dim, feature_dim))\n","\n","    def compute_q_values(self, state):\n","        q_values = np.dot(self.weights, state)\n","        return q_values\n","\n","    def train(self, feature_matrix, reward_matrix):\n","        for _ in range(self.max_iterations):\n","            prev_weights = np.copy(self.weights)\n","            for state_idx in range(self.state_dim):\n","                state = feature_matrix[state_idx]\n","                q_values = self.compute_q_values(state)\n","                best_action_value = np.max(q_values)\n","                for action_idx in range(self.action_dim):\n","                    reward = reward_matrix[action_idx, state_idx]\n","                    bellman_residual = reward + self.gamma * best_action_value - q_values[action_idx]\n","                    self.weights[action_idx] += np.dot(state, bellman_residual)\n","            if np.linalg.norm(prev_weights - self.weights) < self.epsilon:\n","                break\n","\n","    def get_policy(self, feature_matrix):\n","        policy = np.zeros(self.state_dim, dtype=int)\n","        for state_idx in range(self.state_dim):\n","            state = feature_matrix[state_idx]\n","            q_values = self.compute_q_values(state)\n","            policy[state_idx] = np.argmax(q_values)\n","        return policy\n","\n","# API implementation\n","class ApproximatePolicyIteration:\n","    def __init__(self, state_dim, action_dim, feature_dim, gamma=0.9, epsilon=1e-6, max_iterations=1000):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.feature_dim = feature_dim\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.max_iterations = max_iterations\n","        self.weights = np.zeros((action_dim, feature_dim))\n","\n","    def compute_q_values(self, state):\n","        q_values = np.dot(self.weights, state)\n","        return q_values\n","\n","    def compute_value_function(self, feature_matrix):\n","        value_function = np.zeros(self.state_dim)\n","        for state_idx in range(self.state_dim):\n","            state = feature_matrix[state_idx]\n","            q_values = self.compute_q_values(state)\n","            value_function[state_idx] = np.max(q_values)\n","        return value_function\n","\n","    def train(self, feature_matrix, reward_matrix):\n","        for _ in range(self.max_iterations):\n","            prev_weights = np.copy(self.weights)\n","            for _ in range(self.max_iterations):\n","                prev_value_function = self.compute_value_function(feature_matrix)\n","                for state_idx in range(self.state_dim):\n","                    state = feature_matrix[state_idx]\n","                    q_values = self.compute_q_values(state)\n","                    policy = np.argmax(q_values)\n","                    reward = reward_matrix[policy, state_idx]\n","                    bellman_residual = reward + self.gamma * prev_value_function[state_idx] - q_values[policy]\n","                    self.weights[policy] += np.dot(state, bellman_residual)\n","                value_function = self.compute_value_function(feature_matrix)\n","                if np.linalg.norm(prev_value_function - value_function) < self.epsilon:\n","                    break\n","            if np.linalg.norm(prev_weights - self.weights) < self.epsilon:\n","                break\n","\n","    def get_policy(self, feature_matrix):\n","        policy = np.zeros(self.state_dim, dtype=int)\n","        for state_idx in range(self.state_dim):\n","            state = feature_matrix[state_idx]\n","            q_values = self.compute_q_values(state)\n","            policy[state_idx] = np.argmax(q_values)\n","        return policy\n","\n","# Example Usage and Output:\n","\n","# Define example data\n","state_dim = 5\n","action_dim = 2\n","feature_dim = 3\n","gamma = 0.9\n","epsilon = 1e-6\n","max_iterations = 1000\n","\n","feature_matrix = np.random.rand(state_dim, feature_dim)\n","reward_matrix = np.random.rand(action_dim, state_dim)\n","\n","# Instantiate and train AVI\n","avi = ApproximateValueIteration(state_dim, action_dim, feature_dim, gamma, epsilon, max_iterations)\n","avi.train(feature_matrix, reward_matrix)\n","\n","# Obtain AVI policy\n","avi_policy = avi.get_policy(feature_matrix)\n","print(\"Approximate Value Iteration (AVI) Policy:\")\n","print(avi_policy)\n","\n","# Instantiate and train API\n","api = ApproximatePolicyIteration(state_dim, action_dim, feature_dim, gamma, epsilon, max_iterations)\n","api.train(feature_matrix, reward_matrix)\n","\n","# Obtain API policy\n","api_policy = api.get_policy(feature_matrix)\n","print(\"Approximate Policy Iteration (API) Policy:\")\n","print(api_policy)\n"]}]}