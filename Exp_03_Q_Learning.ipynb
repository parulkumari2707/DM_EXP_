{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJSC4PaOkClhKm4dqSf3m3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9iE2cgobQI4c","executionInfo":{"status":"ok","timestamp":1716749693933,"user_tz":-330,"elapsed":13460,"user":{"displayName":"Mohammed Mandekar","userId":"14176810042991264297"}},"outputId":"541250b8-788c-437d-ca3d-355321e24635"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]},{"output_type":"stream","name":"stdout","text":["Learned Q-table:\n","[[ 0.          0.          0.          0.          0.          0.        ]\n"," [-2.25986783 -2.26362317 -2.26659317 -2.26417271 -2.26026171 -5.66926706]\n"," [-1.76085074 -1.72915016 -1.71616547 -1.72451276 -0.84744202 -5.48619764]\n"," ...\n"," [-1.24876393 -1.15865402 -1.24134319 -1.24096283 -1.96       -4.1691573 ]\n"," [-1.98904589 -1.97654771 -1.99123398 -1.99742189 -4.96471704 -2.8352896 ]\n"," [-0.196      -0.196      -0.196       3.30259252 -1.         -1.        ]]\n","Average Reward: -163.94\n"]}],"source":["import numpy as np\n","import gym\n","\n","# Create the environment\n","env = gym.make('Taxi-v3')\n","\n","# Initialize Q-table with zeros\n","Q = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","# Set hyperparameters\n","alpha = 0.1  # Learning rate\n","gamma = 0.6  # Discount factor\n","epsilon = 0.1  # Exploration rate\n","\n","# Number of episodes\n","episodes = 1000\n","\n","# Q-Learning algorithm\n","for _ in range(episodes):\n","    state = env.reset()\n","    done = False\n","    while not done:\n","        # Epsilon-greedy policy\n","        if np.random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample()  # Exploration\n","        else:\n","            action = np.argmax(Q[state])  # Exploitation\n","\n","        next_state, reward, done, _ = env.step(action)\n","\n","        # Q-value update\n","        old_q_value = Q[state, action]\n","        next_max = np.max(Q[next_state])\n","        new_q_value = (1 - alpha) * old_q_value + alpha * (reward + gamma * next_max)\n","        Q[state, action] = new_q_value\n","\n","        state = next_state\n","\n","# Print the learned Q-table\n","print(\"Learned Q-table:\")\n","print(Q)\n","\n","# Evaluate the learned policy\n","total_rewards = 0\n","episodes = 100\n","for _ in range(episodes):\n","    state = env.reset()\n","    done = False\n","    while not done:\n","        action = np.argmax(Q[state])\n","        state, reward, done, _ = env.step(action)\n","        total_rewards += reward\n","\n","# Average reward over episodes\n","average_reward = total_rewards / episodes\n","print(\"Average Reward:\", average_reward)\n"]}]}