{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMc3vlSkPOLWOzL1mMHkfIp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SRgkfQtROuH5","executionInfo":{"status":"ok","timestamp":1716749295425,"user_tz":-330,"elapsed":4,"user":{"displayName":"Mohammed Mandekar","userId":"14176810042991264297"}},"outputId":"415a14ce-d686-4574-d81e-ffe1f551d456"},"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Policy (Monte Carlo Control): [0 0 0]\n","Q-Table (Monte Carlo Control): [[0.43151488 0.        ]\n"," [0.08937007 0.        ]\n"," [0.56597802 0.        ]]\n"]}],"source":["import numpy as np\n","import random\n","\n","def generate_episode(P, R, policy, n_states, max_steps=100):\n","    episode = []\n","    state = random.choice(range(n_states))\n","    for _ in range(max_steps):\n","        action = policy[state]\n","        next_state = np.random.choice(range(n_states), p=P[action, state])\n","        reward = R[action, state]\n","        episode.append((state, action, reward))\n","        state = next_state\n","        if state == next_state:  # Assuming episode ends when reaching a terminal state\n","            break\n","    return episode\n","\n","def monte_carlo_control(P, R, n_states, n_actions, gamma=0.9, epsilon=0.1, episodes=1000):\n","    Q = np.zeros((n_states, n_actions))\n","    returns = { (s, a): [] for s in range(n_states) for a in range(n_actions) }\n","    policy = np.zeros(n_states, dtype=int)\n","\n","    for _ in range(episodes):\n","        episode = generate_episode(P, R, policy, n_states)\n","        G = 0\n","        for t in reversed(range(len(episode))):\n","            state, action, reward = episode[t]\n","            G = gamma * G + reward\n","            if not any((state == x[0] and action == x[1]) for x in episode[:t]):\n","                returns[(state, action)].append(G)\n","                Q[state, action] = np.mean(returns[(state, action)])\n","                policy[state] = np.argmax(Q[state])\n","\n","    return policy, Q\n","\n","### Utility Functions ###\n","\n","def validate_transition_matrix(P):\n","    assert np.allclose(P.sum(axis=2), 1), \"Transition probabilities must sum to 1.\"\n","\n","def validate_reward_matrix(R, P):\n","    assert R.shape == P.shape[:2], \"Reward matrix dimensions must match the transition matrix.\"\n","\n","def generate_random_mdp(n_states, n_actions):\n","    P = np.zeros((n_actions, n_states, n_states))\n","    for a in range(n_actions):\n","        for s in range(n_states):\n","            P[a, s, :] = np.random.dirichlet(np.ones(n_states))\n","    R = np.random.rand(n_actions, n_states)\n","    return P, R\n","\n","### Example Usage ###\n","\n","# Generate a random MDP\n","n_states = 3\n","n_actions = 2\n","P, R = generate_random_mdp(n_states, n_actions)\n","\n","# Validate the MDP\n","validate_transition_matrix(P)\n","validate_reward_matrix(R, P)\n","\n","# Solve the MDP using Monte Carlo Control\n","policy_mc, Q_mc = monte_carlo_control(P, R, n_states, n_actions)\n","print(\"Optimal Policy (Monte Carlo Control):\", policy_mc)\n","print(\"Q-Table (Monte Carlo Control):\", Q_mc)"]}]}