{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNXkAH3cf0RWJU12cfmwnee"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uB4f2h0qYbSo","executionInfo":{"status":"ok","timestamp":1716751905057,"user_tz":-330,"elapsed":1026,"user":{"displayName":"Mohammed Mandekar","userId":"14176810042991264297"}},"outputId":"686f8851-e7fd-4e88-e815-4c4eb0769b73"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]},{"output_type":"stream","name":"stdout","text":["Average Rewards: 0.0\n"]}],"source":["import numpy as np\n","import random\n","import gym\n","\n","class SARSA:\n","    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.1, max_episodes=1000, max_steps=100):\n","        self.env = env\n","        self.alpha = alpha  # learning rate\n","        self.gamma = gamma  # discount factor\n","        self.epsilon = epsilon  # exploration-exploitation tradeoff\n","        self.max_episodes = max_episodes\n","        self.max_steps = max_steps\n","        self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n","\n","    def choose_action(self, state):\n","        if np.random.uniform(0, 1) < self.epsilon:\n","            return self.env.action_space.sample()  # Explore action space\n","        else:\n","            return np.argmax(self.q_table[state, :])  # Exploit learned values\n","\n","    def update_q_table(self, state, action, reward, next_state, next_action):\n","        predict = self.q_table[state, action]\n","        target = reward + self.gamma * self.q_table[next_state, next_action]\n","        self.q_table[state, action] += self.alpha * (target - predict)\n","\n","    def train(self):\n","        rewards = []\n","        for episode in range(self.max_episodes):\n","            state = self.env.reset()\n","            total_reward = 0\n","            action = self.choose_action(state)\n","            for step in range(self.max_steps):\n","                next_state, reward, done, _ = self.env.step(action)\n","                next_action = self.choose_action(next_state)\n","                self.update_q_table(state, action, reward, next_state, next_action)\n","                total_reward += reward\n","                state = next_state\n","                action = next_action\n","                if done:\n","                    break\n","            rewards.append(total_reward)\n","        return rewards\n","\n","# Create a grid world environment\n","env = gym.make(\"FrozenLake-v1\")\n","\n","# Create an instance of SARSA\n","sarsa_agent = SARSA(env)\n","\n","# Train SARSA\n","rewards = sarsa_agent.train()\n","\n","# Print average rewards\n","print(\"Average Rewards:\", np.mean(rewards))"]}]}