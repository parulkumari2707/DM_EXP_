{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOuyLPVaqbuEUl09EUkCWDa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XkfVgtK_TX_9","executionInfo":{"status":"ok","timestamp":1716751118839,"user_tz":-330,"elapsed":94537,"user":{"displayName":"Mohammed Mandekar","userId":"14176810042991264297"}},"outputId":"20b268b6-ec75-4546-b404-c207edc2de07"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 1 Reward: -1495.760387698824\n","Episode: 2 Reward: -1317.6043848744087\n","Episode: 3 Reward: -1148.501590913999\n","Episode: 4 Reward: -1255.3839254633124\n","Episode: 5 Reward: -944.9861228986489\n","Episode: 6 Reward: -1559.6234125688973\n","Episode: 7 Reward: -1171.1532151987162\n","Episode: 8 Reward: -1729.408661360246\n","Episode: 9 Reward: -1639.4917373002643\n","Episode: 10 Reward: -1371.8198135753485\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","import gym\n","\n","# Actor Model\n","class Actor(tf.keras.Model):\n","    def __init__(self, state_dim, action_dim, action_bound):\n","        super(Actor, self).__init__()\n","        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n","        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n","        self.dense3 = tf.keras.layers.Dense(action_dim, activation='tanh')\n","        self.action_bound = action_bound\n","\n","    def call(self, inputs):\n","        # Reshape the input tensor to have a shape of (batch_size, input_dim)\n","        x = tf.expand_dims(inputs, axis=0)  # Add a batch dimension\n","        x = self.dense1(x)\n","        x = self.dense2(x)\n","        x = self.dense3(x)\n","        return tf.squeeze(x, axis=0)  # Remove the added batch dimension\n","\n","\n","# Critic Model\n","class Critic(tf.keras.Model):\n","    def __init__(self):\n","        super(Critic, self).__init__()\n","        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n","        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n","        self.dense3 = tf.keras.layers.Dense(1)\n","\n","    def call(self, inputs):\n","        # Reshape the input tensor to have a shape of (batch_size, input_dim)\n","        x = tf.expand_dims(inputs, axis=0)  # Add a batch dimension\n","        x = self.dense1(x)\n","        x = self.dense2(x)\n","        x = self.dense3(x)\n","        return tf.squeeze(x, axis=0)  # Remove the added batch dimension\n","\n","\n","# Actor-Critic Agent\n","class ActorCriticAgent:\n","    def __init__(self, state_dim, action_dim, action_bound, gamma=0.99, actor_lr=0.001, critic_lr=0.001):\n","        self.actor = Actor(state_dim, action_dim, action_bound)\n","        self.critic = Critic()\n","        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n","        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)\n","        self.gamma = gamma\n","\n","    def get_action(self, state):\n","        return self.actor(tf.convert_to_tensor([state])).numpy()[0]\n","\n","    def train(self, states, actions, rewards, next_states, dones):\n","        # Compute TD targets\n","        next_q_values = self.critic(tf.convert_to_tensor(next_states, dtype=tf.float32))\n","        targets = rewards + (1 - dones) * self.gamma * next_q_values.numpy().flatten()\n","\n","        # Compute advantages\n","        values = self.critic(tf.convert_to_tensor(states, dtype=tf.float32)).numpy().flatten()\n","        advantages = targets - values\n","\n","        # Train actor\n","        with tf.GradientTape() as tape:\n","            actor_actions = self.actor(tf.convert_to_tensor(states, dtype=tf.float32))\n","            actor_loss = -tf.reduce_mean(self.critic(tf.convert_to_tensor(states, dtype=tf.float32)) * actor_actions)\n","        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n","        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n","\n","        # Train critic\n","        with tf.GradientTape() as tape:\n","            critic_values = self.critic(tf.convert_to_tensor(states, dtype=tf.float32))\n","            critic_loss = tf.reduce_mean(tf.square(targets - critic_values))\n","        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n","        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n","\n","# Example Usage\n","env = gym.make('Pendulum-v1')\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","action_bound = env.action_space.high[0]\n","\n","agent = ActorCriticAgent(state_dim, action_dim, action_bound)\n","\n","episodes = 10\n","for episode in range(episodes):\n","    state = env.reset()\n","    episode_reward = 0\n","    while True:\n","        action = agent.get_action(state)\n","        next_state, reward, done, _ = env.step(action)\n","        agent.train(state, action, reward, next_state, done)\n","        episode_reward += reward\n","        state = next_state\n","        if done:\n","            print(\"Episode:\", episode + 1, \"Reward:\", episode_reward)\n","            break\n"]}]}