{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMfZQa1Y5Mtc61JS4ZzunYY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7RCreCv1XI0G","executionInfo":{"status":"ok","timestamp":1716751519185,"user_tz":-330,"elapsed":689,"user":{"displayName":"Mohammed Mandekar","userId":"14176810042991264297"}},"outputId":"2f8a7bc0-70f2-4948-f842-cd7c85d414a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Policy:\n","State 0: up\n","State 1: up\n","State 2: up\n","State 3: up\n","State 4: up\n","State 5: up\n","State 6: up\n","State 7: up\n","State 8: up\n"]}],"source":["import numpy as np\n","\n","class RTDP:\n","    def __init__(self, state_space, action_space, transition_model, reward_model, gamma=0.9, max_iterations=1000):\n","        self.state_space = state_space\n","        self.action_space = action_space\n","        self.transition_model = transition_model\n","        self.reward_model = reward_model\n","        self.gamma = gamma\n","        self.max_iterations = max_iterations\n","\n","        self.value_function = np.zeros(len(state_space))\n","        self.policy = np.zeros(len(state_space), dtype=int)\n","\n","    def run(self):\n","        for _ in range(self.max_iterations):\n","            for state in self.state_space:\n","                action_values = []\n","                for action in self.action_space:\n","                    next_state = self.transition_model(state, action)\n","                    reward = self.reward_model(state, action, next_state)\n","                    action_value = reward + self.gamma * self.value_function[next_state]\n","                    action_values.append(action_value)\n","                best_action = np.argmax(action_values)\n","                best_value = action_values[best_action]\n","                self.value_function[state] = best_value\n","                self.policy[state] = best_action\n","\n","def transition_model(state, action):\n","    # Simple grid world transition model\n","    if action == 'up':\n","        return state - 3 if state >= 3 else state\n","    elif action == 'down':\n","        return state + 3 if state < 6 else state\n","    elif action == 'left':\n","        return state - 1 if state % 3 != 0 else state\n","    elif action == 'right':\n","        return state + 1 if state % 3 != 2 else state\n","\n","def reward_model(state, action, next_state):\n","    # Simple reward model: -1 for every step\n","    return -1\n","\n","# Define the state and action space\n","state_space = np.arange(9)\n","action_space = ['up', 'down', 'left', 'right']\n","\n","# Create an instance of RTDP\n","rtdp = RTDP(state_space, action_space, transition_model, reward_model)\n","\n","# Run RTDP\n","rtdp.run()\n","\n","# Print the optimal policy\n","print(\"Optimal Policy:\")\n","for i, action in enumerate(rtdp.policy):\n","    print(f\"State {i}: {action_space[action]}\")"]}]}